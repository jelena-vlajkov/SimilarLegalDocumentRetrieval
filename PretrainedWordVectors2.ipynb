{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cf88e7b",
   "metadata": {},
   "source": [
    "<h1> Document Vectorization </h1>\n",
    "\n",
    "This experiment will be based on Google News pretrained Word2Vec and GloVe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c283bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "import string \n",
    "import gensim.downloader\n",
    "import math\n",
    "import requests\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff5ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_DOCUMENTS_TRAIN = 1000\n",
    "NUM_OF_DOCUMENTS_TEST = 500\n",
    "URL = \"https://www.courtlistener.com/api/rest/v3/opinions/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019a6a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document(file_name):\n",
    "    data = \"\"\n",
    "    with open(file_name) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data[\"plain_text\"].replace(\"\\n\", \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c696f790",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = [\"id\", \"document\"])\n",
    "\n",
    "i = 0\n",
    "for file_name in [file for file in os.listdir(\"data/train/\") if file.endswith('.json')]:\n",
    "    try:\n",
    "        document = get_document(\"data/train/\" + file_name)\n",
    "        df.loc[i] = [file_name, document.lower()]\n",
    "        i += 1\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bae8588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(columns = [\"id\", \"document\"])\n",
    "i = 0\n",
    "for file_name in [file for file in os.listdir(\"data/test/\") if file.endswith('.json')]:\n",
    "    try:\n",
    "        document = get_document(\"data/test/\" + file_name)\n",
    "        df_test.loc[i] = [file_name, document.lower()]\n",
    "        i += 1\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5f032d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"train_pretrained.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d876d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"test_pretrained.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1216ccac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174995.json</td>\n",
       "      <td>united states court of appeals  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174996.json</td>\n",
       "      <td>united states court of appeals      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175074.json</td>\n",
       "      <td>united states court of appeals   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>175075.json</td>\n",
       "      <td>united states court of appeals      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175076.json</td>\n",
       "      <td>united states court of appeals      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>198335.json</td>\n",
       "      <td>united states court of appeals\\r ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>198336.json</td>\n",
       "      <td>united states court of appeals\\r  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>198337.json</td>\n",
       "      <td>[not for publication--not to be cited as prec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>198338.json</td>\n",
       "      <td>[not for publication--not to be cited as prece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>198339.json</td>\n",
       "      <td>united states court of appeals\\r ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1396 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                           document\n",
       "0     174995.json                united states court of appeals  ...\n",
       "1     174996.json            united states court of appeals      ...\n",
       "2     175074.json               united states court of appeals   ...\n",
       "3     175075.json            united states court of appeals      ...\n",
       "4     175076.json            united states court of appeals      ...\n",
       "...           ...                                                ...\n",
       "1391  198335.json               united states court of appeals\\r ...\n",
       "1392  198336.json              united states court of appeals\\r  ...\n",
       "1393  198337.json   [not for publication--not to be cited as prec...\n",
       "1394  198338.json  [not for publication--not to be cited as prece...\n",
       "1395  198339.json               united states court of appeals\\r ...\n",
       "\n",
       "[1396 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1a65d",
   "metadata": {},
   "source": [
    "<h2> Google News Word2Vec </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "237cede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84587f37",
   "metadata": {},
   "source": [
    "<h2> GloVe </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f957602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "775d71e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_other_vars(measure, avg, wordEmb):\n",
    "    if measure == cosine_similarity:\n",
    "        strtype, asc = \"cosine similarity\", -1\n",
    "    elif measure == euclidean_distances:\n",
    "        strtype, asc = \"euclidean distance\", 1\n",
    "    else:\n",
    "        strtype, asc = \"manhattan distance\", 1\n",
    "        \n",
    "    if (avg == True) and (wordEmb == word2vec):\n",
    "        dftrain = df_avg\n",
    "    elif (avg == True) and (wordEmb == glove):\n",
    "        dftrain = df_avg_glove\n",
    "    elif (avg == False) and (wordEmb == glove):\n",
    "        dftrain = df_sum_glove\n",
    "    else:\n",
    "        dftrain = df_sum\n",
    "    return strtype, asc, dftrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bef810f",
   "metadata": {},
   "source": [
    "<h3> Function for fitting training sets for average and sum of vectors </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07ec1edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_training(model):\n",
    "    df_sum = pd.DataFrame()\n",
    "    df_avg = pd.DataFrame()\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        document = row['document'].translate(str.maketrans('', '', string.punctuation)).lstrip().rstrip()\n",
    "        words = document.split()\n",
    "        word_vec = np.zeros((300, ))\n",
    "        for word in words:\n",
    "            if word.lower() in model.key_to_index :\n",
    "                word_vec += model[word.lower()]\n",
    "            elif word in model.key_to_index :\n",
    "                word_vec += model[word]\n",
    "        if len(words) > 0:\n",
    "            word_vec_avg = word_vec / len(words)\n",
    "        else:\n",
    "            word_vec_avg = 0\n",
    "        word_vec = pd.Series(word_vec)\n",
    "        df_sum = df_sum.append(pd.Series(word_vec), ignore_index=True)\n",
    "        df_avg = df_avg.append(pd.Series(word_vec_avg), ignore_index=True)\n",
    "    return df_sum, df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5006a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx(type, n, document, model, avg):\n",
    "    df_q = pd.DataFrame()\n",
    "    document = document.translate(str.maketrans('', '', string.punctuation)).lstrip().rstrip()\n",
    "    words = document.split()\n",
    "    word_vec = np.zeros((300, ))\n",
    "    \n",
    "    for word in words:\n",
    "        if word.lower() in model.key_to_index:\n",
    "            word_vec += model[word.lower()]\n",
    "        elif word in model.key_to_index:\n",
    "            word_vec += model[word]\n",
    "            \n",
    "    if avg == True:\n",
    "        word_vec = word_vec/len(words)\n",
    "        \n",
    "    df_q = df_q.append(pd.Series(word_vec), ignore_index=True)\n",
    "\n",
    "    strtype, asc, dftrain = get_other_vars(type, avg, model)\n",
    "    distances = type(df_q, dftrain).flatten()\n",
    "    indexes = np.argsort(distances)[::asc]\n",
    "    indexes = indexes[:n]\n",
    "    \n",
    "    return indexes, strtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0509b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_df(dict, csv_name):\n",
    "    df_to_save = pd.DataFrame(dict.items())\n",
    "    df_to_save.to_csv(csv_name + \".csv\", sep='\\t')\n",
    "\n",
    "def df_to_dict(csv_name):\n",
    "    data_frame = pd.read_csv(csv_name, sep = '\\t')\n",
    "    dict = data_frame.set_index('0').T.to_dict('list')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fbb1fb",
   "metadata": {},
   "source": [
    "<h3>  Inverse document frequency - IDF </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb14069b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp/ipykernel_3752/1652140756.py:7: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  dict = data_frame.set_index('0').T.to_dict('list')\n"
     ]
    }
   ],
   "source": [
    "idf = df_to_dict(\"idf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "319e4fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf = {}\n",
    "# for i in range(len(df.iloc[:, 1])):\n",
    "#     tokens = df.iloc[i, 1].translate(str.maketrans('', '', string.punctuation)).lstrip().rstrip().split()\n",
    "#     for w in tokens:\n",
    "#         try:\n",
    "#             idf[w.lower()].add(i)\n",
    "#         except:\n",
    "#             idf[w.lower()] = {i}\n",
    "\n",
    "# for key, value in idf2.items():\n",
    "#     idf[key] = math.log(len(df.iloc[:, 0]) / (len(value)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "915f3bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_idf(type, n, document, model, avg):\n",
    "    df_q = pd.DataFrame()\n",
    "    document = document.translate(str.maketrans('', '', string.punctuation)).lstrip().rstrip()\n",
    "    words = document.split()\n",
    "    word_vec = np.zeros((300, ))\n",
    "    for word in words:\n",
    "        idfCoeff = 1\n",
    "        if word.lower() in idf:\n",
    "            idfCoeff = idf[word.lower()][1]\n",
    "        if word.lower() in model.key_to_index:\n",
    "            word_vec += model[word.lower()]*idfCoeff\n",
    "        elif word in model.key_to_index:\n",
    "            word_vec += model[word]*idfCoeff\n",
    "            \n",
    "    if avg == True:\n",
    "        word_vec = word_vec / len(words)\n",
    "        \n",
    "    df_q = df_q.append(pd.Series(word_vec), ignore_index=True)\n",
    "\n",
    "    strtype, asc, dftrain = get_other_vars(type, avg, model)\n",
    "\n",
    "    distances = type(df_q, dftrain).flatten()\n",
    "    indexes = np.argsort(distances)[::asc]\n",
    "    indexes = indexes[:n]\n",
    "    \n",
    "    return indexes, strtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025a571c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f2aee13",
   "metadata": {},
   "source": [
    "<h3>  Part-of-speech - POS </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee9ec5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe7ba8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp/ipykernel_3752/1652140756.py:7: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  dict = data_frame.set_index('0').T.to_dict('list')\n"
     ]
    }
   ],
   "source": [
    "pos = df_to_dict(\"pos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dee88e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos = {}\n",
    "\n",
    "# for sentence in df.iloc[:, 1]:\n",
    "#     for token in nlp(sentence.lower()):\n",
    "#         pos[token.text] = {token.pos_ : None}\n",
    "        \n",
    "# for sentence in df_test.iloc[:, 1]:\n",
    "#     for token in nlp(sentence.lower()):\n",
    "#         pos[token.text] = {token.pos_ : None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a957857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word, posTagAndVal in pos.items():\n",
    "    \n",
    "#     for tag in posTagAndVal:\n",
    "#         if tag == \"NOUN\":\n",
    "#             posTagAndVal[tag] = 0.7\n",
    "#         elif tag == \"PROPN\": \n",
    "#             posTagAndVal[tag] = 0.9\n",
    "#         elif tag == \"ADJ\":\n",
    "#             posTagAndVal[tag] = 0.8\n",
    "#         elif tag == \"ADV\":\n",
    "#             posTagAndVal[tag] = 0.7\n",
    "#         elif tag == \"VERB\":\n",
    "#             posTagAndVal[tag] = 0.6\n",
    "#         elif tag == \"SPACE\" or tag == \"PUNCT\":\n",
    "#             posTagAndVal[tag] = 0\n",
    "#         else:\n",
    "#             posTagAndVal[tag] = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "065dd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_pos(type, n, document, model, avg):\n",
    "    df_q = pd.DataFrame()\n",
    "    document = document.translate(str.maketrans('', '', string.punctuation)).lstrip().rstrip()\n",
    "    words = document.split()\n",
    "    word_vec = np.zeros((300, ))\n",
    "    for word in words:\n",
    "        posCoeff = 1\n",
    "        if word.lower() in pos:\n",
    "            posCoeff = next(iter(ast.literal_eval(pos[word.lower()][1]).values()))\n",
    "        if word.lower() in model.key_to_index:\n",
    "            word_vec += model[word.lower()]*posCoeff\n",
    "        elif word in model.key_to_index:\n",
    "            word_vec += model[word]*posCoeff\n",
    "            \n",
    "    if avg == True:\n",
    "        word_vec = word_vec / len(words)\n",
    "        \n",
    "    df_q = df_q.append(pd.Series(word_vec), ignore_index=True)\n",
    "\n",
    "    strtype, asc, dftrain = get_other_vars(type, avg, model)\n",
    "\n",
    "    distances = type(df_q, dftrain).flatten()\n",
    "    indexes = np.argsort(distances)[::asc]\n",
    "    indexes = indexes[:n]\n",
    "    \n",
    "    return indexes, strtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeeb886",
   "metadata": {},
   "source": [
    "<h3>   Named Entity Recognition - NER </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d0e6157",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = df_to_dict(\"ner.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1417d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner = {}\n",
    "\n",
    "# for sentence in df.iloc[:, 1]:\n",
    "#     for token in nlp(sentence.lower()).ents:\n",
    "#         ner[token.text] = {token.label_: None}\n",
    "        \n",
    "# for sentence in df_test.iloc[:, 1]:\n",
    "#     for token in nlp(sentence.lower()).ents:\n",
    "#         ner[token.text] = {token.label_: None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e126f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word, nerTagAndVal in ner.items():\n",
    "    \n",
    "#     for tag in nerTagAndVal:\n",
    "#         if (tag == \"LOCATION\") or (tag == \"ORG\") or (tag == \"NORP\") or (tag == \"MONEY\") or (tag == \"WORK_OF_ART\") or (tag == \"LAW\"):\n",
    "#             nerTagAndVal[tag] = 1.75\n",
    "#         elif (tag == \"GPE\") or (tag == \"DATE\") or (tag == \"PERSON\") or (tag == \"FAC\"): \n",
    "#             nerTagAndVal[tag] = 1.5\n",
    "#         elif (tag == \"ORDINAL\") or (tag == \"CARDINAL\") or (tag == \"PRODUCT\") or (tag == \"PERCENT\") or (tag == \"TIME\"):\n",
    "#             nerTagAndVal[tag] = 1.2\n",
    "#         else:\n",
    "#             nerTagAndVal[tag] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03c7203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_ner(type, n, document, model, avg):\n",
    "    df_q = pd.DataFrame()\n",
    "    document = document.translate(str.maketrans('', '', string.punctuation)).lstrip().rstrip()\n",
    "    words = document.split()\n",
    "    word_vec = np.zeros((300, ))\n",
    "    for word in words:\n",
    "        nerCoeff = 1\n",
    "        if word.lower() in ner:\n",
    "            nerCoeff = next(iter(ast.literal_eval(ner[word.lower()][1]).values()))\n",
    "        if word.lower() in model.key_to_index:\n",
    "            word_vec += model[word.lower()]*nerCoeff\n",
    "        elif word in model.key_to_index:\n",
    "            word_vec += model[word]*nerCoeff\n",
    "            \n",
    "    if avg == True:\n",
    "        word_vec = word_vec / len(words)\n",
    "        \n",
    "    df_q = df_q.append(pd.Series(word_vec), ignore_index=True)\n",
    "\n",
    "    strtype, asc, dftrain = get_other_vars(type, avg, model)\n",
    "\n",
    "    distances = type(df_q, dftrain).flatten()\n",
    "    indexes = np.argsort(distances)[::asc]\n",
    "    indexes = indexes[:n]\n",
    "    \n",
    "    return indexes, strtype "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a78b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_pos_ner(type, n, document, model, avg):\n",
    "    df_q = pd.DataFrame()\n",
    "    document = document.translate(str.maketrans('', '', string.punctuation)).lstrip().rstrip()\n",
    "    words = document.split()\n",
    "    word_vec = np.zeros((300, ))\n",
    "    for word in words:\n",
    "        posCoeff = 1\n",
    "        nerCoeff = 1\n",
    "        if word.lower() in ner:\n",
    "            nerCoeff = next(iter(ast.literal_eval(ner[word.lower()][1]).values()))\n",
    "        if word.lower() in pos:\n",
    "            posCoeff = next(iter(ast.literal_eval(pos[word.lower()][1]).values()))\n",
    "        if word.lower() in model.key_to_index:\n",
    "            word_vec += model[word.lower()]*nerCoeff*posCoeff\n",
    "        elif word in model.key_to_index:\n",
    "            word_vec += model[word]*nerCoeff*posCoeff\n",
    "            \n",
    "    if avg == True:\n",
    "        word_vec = word_vec/len(words)\n",
    "        \n",
    "    df_q = df_q.append(pd.Series(word_vec), ignore_index=True)\n",
    "\n",
    "    strtype, asc, dftrain = get_other_vars(type, avg, model)\n",
    "\n",
    "    distances = type(df_q, dftrain).flatten()\n",
    "    indexes = np.argsort(distances)[::asc]\n",
    "    indexes = indexes[:n]\n",
    "    \n",
    "    return indexes, strtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1e7efb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_pos_idf(type, n, document, model, avg):\n",
    "    df_q = pd.DataFrame()\n",
    "    document = document.translate(str.maketrans('', '', string.punctuation)).lstrip().rstrip()\n",
    "    words = document.split()\n",
    "    word_vec = np.zeros((300, ))\n",
    "    for word in words:\n",
    "        idfCoeff = 1\n",
    "        posCoeff = 1\n",
    "        if word.lower() in pos:\n",
    "            posCoeff = next(iter(ast.literal_eval(pos[word.lower()][1]).values()))\n",
    "        if word.lower() in idf:\n",
    "            idfCoeff = idf[word.lower()][1]\n",
    "        if word.lower() in model.key_to_index:\n",
    "            word_vec += model[word.lower()]*posCoeff*idfCoeff\n",
    "        elif word in model.key_to_index:\n",
    "            word_vec += model[word]*posCoeff*idfCoeff\n",
    "            \n",
    "    if avg == True:\n",
    "        word_vec = word_vec/len(words)\n",
    "        \n",
    "    df_q = df_q.append(pd.Series(word_vec), ignore_index=True)\n",
    "\n",
    "    strtype, asc, dftrain = get_other_vars(type, avg, model)\n",
    "\n",
    "    distances = type(df_q, dftrain).flatten()\n",
    "    indexes = np.argsort(distances)[::asc]\n",
    "    indexes = indexes[:n]\n",
    "    \n",
    "    return indexes, strtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac936287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_ner_idf(type, n, document, model, avg):\n",
    "    df_q = pd.DataFrame()\n",
    "    document = document.translate(str.maketrans('', '', string.punctuation)).lstrip().rstrip()\n",
    "    words = document.split()\n",
    "    word_vec = np.zeros((300, ))\n",
    "    for word in words:\n",
    "        idfCoeff = 1\n",
    "        nerCoeff = 1\n",
    "        if word.lower() in ner:\n",
    "            nerCoeff = next(iter(ast.literal_eval(ner[word.lower()][1]).values()))\n",
    "        if word.lower() in idf:\n",
    "            idfCoeff = idf[word.lower()][1]\n",
    "        if word.lower() in model.key_to_index:\n",
    "            word_vec += model[word.lower()]*nerCoeff*idfCoeff\n",
    "        elif word in model.key_to_index:\n",
    "            word_vec += model[word]*nerCoeff*idfCoeff\n",
    "            \n",
    "    if avg == True:\n",
    "        word_vec = word_vec/len(words)\n",
    "        \n",
    "    df_q = df_q.append(pd.Series(word_vec), ignore_index=True)\n",
    "\n",
    "    strtype, asc, dftrain = get_other_vars(type, avg, model)\n",
    "\n",
    "    distances = type(df_q, dftrain).flatten()\n",
    "    indexes = np.argsort(distances)[::asc]\n",
    "    indexes = indexes[:n]\n",
    "    \n",
    "    return indexes, strtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5c90f",
   "metadata": {},
   "source": [
    "<h3> The Experiment </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04d0c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordEmbs = [word2vec, glove]\n",
    "functions = [get_idx, get_idx_idf, get_idx_pos, get_idx_ner, \n",
    "             get_idx_pos_ner, get_idx_pos_idf, get_idx_ner_idf]\n",
    "avgs = [True, False]\n",
    "measures = [cosine_similarity]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02329e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum, df_avg = fit_training(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ed6ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum_glove, df_avg_glove = fit_training(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "abd92a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank(type, n, model, avg, function):\n",
    "    results = pd.DataFrame(columns = [\"verdict\", \"indexes\"])\n",
    "    indexes = []\n",
    "    name = \"\"\n",
    "    for i, document in enumerate(df_test.iloc[:, 1]):\n",
    "        indexes, strtype = function(type, n, document, model, avg)\n",
    "        results = results.append(\n",
    "            { \"verdict\" : df_test.iloc[i, 0], \n",
    "              \"indexes\" : indexes}, ignore_index=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "799078cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_rank(cosine_similarity, 100, word2vec, False, get_idx)\n",
    "results.to_csv(\"results/w2v_sum.csv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d58f577",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3752/2292204719.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_rank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_idx_idf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"results/w2v_sum_idf.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3752/718895318.py\u001b[0m in \u001b[0;36mget_rank\u001b[1;34m(type, n, model, avg, function)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mindexes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         results = results.append(\n\u001b[0;32m      8\u001b[0m             { \"verdict\" : df_test.iloc[i, 0], \n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3752/1569357060.py\u001b[0m in \u001b[0;36mget_idx_idf\u001b[1;34m(type, n, document, model, avg)\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0midfCoeff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mword_vec\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0midfCoeff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mword_vec\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0midfCoeff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = get_rank(cosine_similarity, 100, word2vec, False, get_idx_idf)\n",
    "results.to_csv(\"results/w2v_sum_idf.csv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b143b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_rank(cosine_similarity, 100, word2vec, False, get_idx_pos)\n",
    "results.to_csv(\"results/w2v_sum_pos.csv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34feca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_rank(cosine_similarity, 100, word2vec, False, get_idx_ner)\n",
    "results.to_csv(\"results/w2v_sum_ner.csv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f91752",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_rank(cosine_similarity, 100, word2vec, False, get_idx_pos_ner)\n",
    "results.to_csv(\"results/w2v_sum_pos_ner.csv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd8421",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_rank(cosine_similarity, 100, word2vec, False, get_idx_ner_idf)\n",
    "results.to_csv(\"results/w2v_sum_ner_idf.csv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b5b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_rank(cosine_similarity, 100, glove, False, get_idx)\n",
    "results.to_csv(\"results/glove_sum.csv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_rank(cosine_similarity, 100, glove, False, get_idx_idf)\n",
    "results.to_csv(\"results/glove_sum_idf.csv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375d7e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_rank(cosine_similarity, 100, glove, False, get_idx_pos)\n",
    "results.to_csv(\"results/glove_sum_pos.csv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f595abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_rank(cosine_similarity, 100, glove, False, get_idx_ner)\n",
    "results.to_csv(\"results/glove_sum_ner.csv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06ff3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_rank(cosine_similarity, 100, glove, False, get_idx_pos_ner)\n",
    "results.to_csv(\"results/glove_sum_pos_ner.csv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ccdab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_rank(cosine_similarity, 100, glove, False, get_idx_ner_idf)\n",
    "results.to_csv(\"results/glove_sum_ner_idf.csv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5487a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5de9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8871d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearch():\n",
    "    i = 0\n",
    "    name = \"\"\n",
    "    for wordEmb in wordEmbs:\n",
    "        for function in functions:\n",
    "            for avg in avgs:\n",
    "                for measure in measures:\n",
    "                    indexes = get_rank(measure, 100, wordEmb, avg, function)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f559898f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
