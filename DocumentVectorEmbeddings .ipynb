{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d37e439",
   "metadata": {},
   "source": [
    "<h1> Document Vector Embeddings </h1>\n",
    "\n",
    "Initial experiment will be perfomed based on the experiment by Sugathadasa et al. [https://arxiv.org/pdf/1805.10685.pdf]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9fe08e",
   "metadata": {},
   "source": [
    "<h2> Text Preprocessing </h2>\n",
    "\n",
    "First step is to create a <i> document corpus </i> which is a subset of most important sentences in each document. We can do that by implementing the <i> PageRank </i> algorithm. Before we do that, we need to preprocess the document by cleaning the text of unwanted charachters and common words. We used lemmatization and case-folding to lowercase as first steps in cleaning the documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834644a4",
   "metadata": {},
   "source": [
    "<h5> Required libraries </h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b5f1e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38d43d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import nltk.data\n",
    "import pandas as pd\n",
    "from text_rank import analyze \n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7dcafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for CourtListener data\n",
    "\n",
    "URL = \"https://www.courtlistener.com/api/rest/v3/opinions/1\"\n",
    "\n",
    "r = requests.get(url = URL)\n",
    "data = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "135a8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "verdict_text = data[\"plain_text\"]\n",
    "verdict_text = verdict_text.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b4830f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemma Tokenizer called by TfIdfVectorizer\n",
    "\n",
    "class LemmaTokenizer():\n",
    "    def __init__(self):\n",
    "        self.spacynlp = en_core_web_md.load()\n",
    "    def __call__(self, doc):\n",
    "        nlpdoc = self.spacynlp(doc)\n",
    "        nlpdoc = [token.lemma_.lower() for token in nlpdoc if (not token.is_punct)]\n",
    "        return nlpdoc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc5516",
   "metadata": {},
   "source": [
    "<h2> <i> TextRank </i> algorithm </h2>\n",
    "\n",
    "<i> TextRank </i> algorithm will be implemented based on the work of Mihalcea et al [https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf]. <br>\n",
    "We use this algorithm for extracting \"most valuable\" sentences in a document.  <br> <br>\n",
    "<i> TextRank </i> algorithm is implemented in a python script named <i> text_rank.py </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d9de2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sentences = analyze(verdict_text, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fefb93a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Although the court did not specifically reference the factors that the appellant now highlights, the sentencing transcript, read as a whole, evinces a sufficient weighing of the section 3553(a) factors.',\n",
       " \"In this venue, the appellant does not challenge any of these rulings but, rather, accepts the district court's calculation of the guideline sentencing range (GSR): 70-87 months.\",\n",
       " 'That suggestion, however, is grounded in a misreading of the statute.1 This provision applies only when the span of the GSR, measured from the low end to the high end, is greater than 24 months.',\n",
       " 'When a sentencing appeal follows a guilty plea, \"we glean the relevant facts from the change-of-plea colloquy, the unchallenged portions of the presentence investigation report (PSI Report), and the record of the disposition hearing.\"',\n",
       " \"Citing the Court's follow-on decision in Nelson v. United States, 129 S. Ct. 890 (2009) (per curiam), the appellant labors to convince us that the court below transgressed this tenet.\",\n",
       " 'In this regard, the appellant invokes a statute providing that, in a federal criminal case, a sentencing court must \"state in open court the reasons for its imposition of the particular sentence.\"',\n",
       " 'Jiménez- Beltre, 440 F.3d at 519; see also Rita, 551 U.S. at 358 (holding -10- that where the sentencing court imposed a within-the-range sentence and the record indicates that the court heard the defendant\\'s arguments and considered the supporting evidence, it may be inferred that \"[t]he judge simply found the[] circumstances insufficient to warrant a sentence lower than the Guidelines range\").',\n",
       " 'The authorities arrested the appellant in Tampa, Florida, on May 9, 2008.',\n",
       " 'To this question, defense counsel explicated, in some detail, that during the years intervening between the offense conduct and the arrest, the appellant had checked himself into a rehabilitation facility, moved to Florida, forsook his criminal ways, and obtained gainful employment.',\n",
       " 'In this appeal, the appellant argues that the sentencing court committed reversible error by (i) presuming the reasonableness of the GSR; (ii) neglecting sufficiently to explain the sentence imposed; and (iii) failing to consider mitigating factors favoring a below-the-range sentence.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " \"Read as a whole, the sentencing transcript makes manifest not only the court's awareness that the GSR was merely an initial benchmark, but also its conclusion that the circumstances of the case made it appropriate to hew to that benchmark in fashioning the appellant's sentence.\",\n",
       " \"Here, the sentencing court heard about a myriad of circumstances, including the appellant's relative culpability and efforts at rehabilitation.\",\n",
       " 'It also ruled that, for sentencing purposes, it would hold the appellant responsible for only the cash contained in the two bundles ($204,440), even though the conspiracy as a whole had laundered a much greater sum (approximately $1,839,208).',\n",
       " 'at 41; see also Rita v. United States, 551 U.S. 338, 356-57 (2007) (\"[W]hen a judge decides simply to apply the Guidelines to a particular case, doing so will not necessarily require lengthy explanation.\").',\n",
       " \"We approach these claims of error mindful that, in the wake of the Supreme Court's landmark decision in United States v. Booker, 543 U.S. 220, 245 (2005), we have encouraged the district courts to follow a specifically delineated roadmap when sentencing under the now- advisory federal sentencing guidelines: [A] sentencing court ordinarily should begin by calculating the applicable guideline sentencing range; then determine whether or not any departures are in order; then mull the factors delineated in 18 U.S.C.\",\n",
       " 'During the sentencing proceedings, defense counsel briefly mentioned that the appellant was a former heroin addict who, since committing the offense of conviction, had rehabilitated himself.',\n",
       " 'In the end, the court imposed a mid-range sentence of 78 months in prison.',\n",
       " 'Among other things, the codefendant pleaded guilty in accordance with a negotiated plea agreement and, therefore, was not in that respect situated similarly to the appellant.',\n",
       " 'There, the Supreme Court admonished that even though the federal sentencing guidelines \"should be the starting point and the initial benchmark\" for constructing a sentence, the sentencing court \"may not presume that the Guidelines range is reasonable.\"',\n",
       " 'Over the course of several calls, most of which were recorded, the appellant and the informant agreed to meet and consummate the transaction.',\n",
       " 'As a subset of this argument, the appellant suggests that 18 U.S.C.',\n",
       " 'On October 15, 2004, the appellant called a known member of a drug cartel to arrange for the delivery of \"two bundles.\"',\n",
       " 'That taxonomy includes \"failing to calculate (or improperly calculating) the Guidelines range, treating the Guidelines as mandatory, failing to consider the 18 U.S.C.',\n",
       " 'Unfortunately for the appellant, the person to whom he reached out doubled in brass as a confidential informant for the Federal Bureau of Investigation (FBI).',\n",
       " 'In the case at hand, the appellant did not interpose an objection as to any of the procedures that he now seeks to challenge.',\n",
       " \"Although it is true that the district court did not explicitly address each of the appellant's arguments for a below-the-range sentence, the court was not required to offer that level of elucidation.\",\n",
       " 'The touchstone for our analysis is the decision in Gall v. United States, 552 U.S. 38 (2007).',\n",
       " 'This is especially true where, as here, a court prescribes a sentence that falls within the GSR.',\n",
       " 'For the reasons elucidated above, we conclude that the sentencing in this case was free from error, plain or otherwise.',\n",
       " 'On May 3, 2007, a federal grand jury sitting in the District of Puerto Rico returned a six-count indictment against the appellant and others.',\n",
       " \"In the appellant's view, these actions demonstrate the sentencing court's embrace of the very presumption of reasonableness that Gall forbids.\",\n",
       " 'The district court convened the disposition hearing on November 13, 2008.',\n",
       " 'And the fact that the court stated that it had considered all the section 3553(a) factors is entitled to some weight.',\n",
       " '§ 3553(a) as well as any other relevant considerations; and, finally, determine what sentence, whether -5- within, above, or below the guideline sentencing range, appears appropriate.',\n",
       " 'Following his rendition to Puerto Rico, the appellant entered a plea of guilty to the three counts against him (one of which was dismissed at sentencing).',\n",
       " 'There, the district court unequivocally declared at sentencing that \"the Guidelines are considered presumptively reasonable.\"',\n",
       " 'At that time the appellant effected delivery of the \"two bundles\" to the informant.',\n",
       " 'The court deemed applicable a six-level sentencing enhancement after finding that \"the defendant knew or believed that any of the laundered funds were the proceeds of, or were intended to promote .',\n",
       " 'The only rejoinder was from defense counsel, who requested that the sentence be served in \"the Tampa area.\"',\n",
       " 'See, e.g., United States v. Pacheco, 489 F.3d 40, 44 (1st Cir.',\n",
       " 'See, e.g., United States v. Almenas, 553 F.3d 27, 36 (1st Cir.',\n",
       " 'Yet, when a defendant fails to preserve an objection below, the plain error standard supplants the customary standard of review.',\n",
       " 'See, e.g., United States v. Smith, 531 F.3d 109, 112 (1st Cir.',\n",
       " 'Here, the sentencing court made no such declaration.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a59da7b",
   "metadata": {},
   "source": [
    "<h2> Text processing after <i> TextRank </i> algorithm </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8622b9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13380/2673331606.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1275\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m         \"\"\"\n\u001b[1;32m-> 1277\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \"\"\"\n\u001b[1;32m-> 1334\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \"\"\"\n\u001b[1;32m-> 1334\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1322\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m             \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1324\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1325\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \"\"\"\n\u001b[0;32m   1364\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msentence1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m             \u001b[0msentence1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msentence2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1338\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1339\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"after_tok\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# Because of the TextRank algorithm, we have to split the document into sentences to create the document corpus \n",
    "# (document corpus is the k most important sentences after applying TextRank algorithm)\n",
    "\n",
    "\n",
    "sentences = tokenizer.tokenize(sorted_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb457b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_tokenizer = LemmaTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95993d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_text = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    one_sentence = lemma_tokenizer(sentence)\n",
    "    lemmatized_text.append(one_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d424e630",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors = []\n",
    "vectorizer = TfidfVectorizer(stop_words = \"english\")\n",
    "for sentence in lemmatized_text:\n",
    "    try:\n",
    "        tfidf_vectors.append(vectorizer.fit_transform(sentence))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a871b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
