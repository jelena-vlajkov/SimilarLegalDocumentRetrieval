{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d37e439",
   "metadata": {},
   "source": [
    "<h1> Document Vector Embeddings </h1>\n",
    "\n",
    "Initial experiment will be perfomed based on the experiment by Sugathadasa et al. [https://arxiv.org/pdf/1805.10685.pdf]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9fe08e",
   "metadata": {},
   "source": [
    "<h2> Text Preprocessing </h2>\n",
    "\n",
    "First step is to create a <i> document corpus </i> which is a subset of most important sentences in each document. We can do that by implementing the <i> PageRank </i> algorithm. Before we do that, we need to preprocess the document by cleaning the text of unwanted charachters and common words. We used lemmatization and case-folding to lowercase as first steps in cleaning the documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834644a4",
   "metadata": {},
   "source": [
    "<h5> Required libraries </h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92b5f1e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d38d43d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import nltk.data\n",
    "import pandas as pd\n",
    "from text_rank import analyze \n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "import en_core_web_sm\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "de293030",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_DOCUMENTS = 5\n",
    "NUM_OF_SENTENCES = 50\n",
    "NUM_OF_CHARACTERS = 10\n",
    "\n",
    "URL = \"https://www.courtlistener.com/api/rest/v3/opinions/\"\n",
    "\n",
    "RUN_TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73e66ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document(file_name):\n",
    "    data = \"\"\n",
    "    with open(file_name) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data[\"plain_text\"].replace(\"\\n\", \" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc5516",
   "metadata": {},
   "source": [
    "<h2> <i> TextRank </i> algorithm </h2>\n",
    "\n",
    "<i> TextRank </i> algorithm will be implemented based on the work of Mihalcea et al [https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf]. <br>\n",
    "We use this algorithm for extracting \"most valuable\" sentences in a document.  <br> <br>\n",
    "<i> TextRank </i> algorithm is implemented in a python script named <i> text_rank.py </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d9de2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_textrank(text):\n",
    "    sorted_sentences = analyze(text, NUM_OF_SENTENCES)\n",
    "    return sorted_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01429612",
   "metadata": {},
   "source": [
    "Sentences that are shorter than N characters should be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a59da7b",
   "metadata": {},
   "source": [
    "<h2> Text processing after <i> TextRank </i> algorithm </h2>\n",
    "\n",
    "After the <i> TextRank </i> algorithm we apply lemmatization to each word in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33adbf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemma Tokenizer called by TfIdfVectorizer\n",
    "\n",
    "class LemmaTokenizer():\n",
    "    def __init__(self):\n",
    "        self.spacynlp = spacy.load('en_core_web_sm')\n",
    "    def __call__(self, doc):\n",
    "        nlpdoc = self.spacynlp(doc)\n",
    "        nlpdoc = [token.lemma_.lower() for token in nlpdoc if (not token.is_punct)]\n",
    "        return nlpdoc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87b75121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_list2str(s): \n",
    "    str1 = \"\" \n",
    "    for ele in s: \n",
    "        str1 += \" \" + ele  \n",
    "    return str1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c6c8a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatized_txt2str(s):\n",
    "    str1 = \"\" \n",
    "    for ele in s: \n",
    "        for ele2 in ele:\n",
    "            if ele2.isspace():\n",
    "                continue\n",
    "            str1 += \" \" + ele2  \n",
    "    return str1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c35681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_tokenizer = LemmaTokenizer()\n",
    "def lemmatize(sentences):\n",
    "    # Because of the TextRank algorithm, we have to split the document into sentences to create the document corpus \n",
    "    # (document corpus is the k most important sentences after applying TextRank algorithm)\n",
    "\n",
    "\n",
    "    sentences = tokenizer.tokenize(sorted_list2str(sentences))\n",
    "    sentences = [x for x in sentences if len(x) > NUM_OF_CHARACTERS]\n",
    "    lemmatized_text = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        one_sentence = lemma_tokenizer(sentence)\n",
    "        lemmatized_text.append(one_sentence)\n",
    "    lemmatized_text = lemmatized_txt2str(lemmatized_text)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3da9d0",
   "metadata": {},
   "source": [
    "<h2> Apply TF-IDF </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d424e630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_vectorizer(df):\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words = \"english\")\n",
    "    tfidf_vector = tfidf_vectorizer.fit_transform(df.iloc[:, 1].values.astype('U').tolist())\n",
    "    tfidf_df = pd.DataFrame(tfidf_vector.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "        \n",
    "    return tfidf_vectorizer, tfidf_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e675299",
   "metadata": {},
   "source": [
    "<h2> Global Term Frequency </h2>\n",
    "\n",
    "To see how important a word is in the whole dataset, we calculate GTF_IDF matrix applying the formula below:\n",
    "\n",
    "GTF_IDF = TF_IDF * sum(TF_IDF) / NUM_OF_DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a72e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_gtfidf(tf_df):\n",
    "    sum_of_idfs = tfidf_df.sum(axis = 0)\n",
    "    for i in range(len(tfidf_df.columns)):\n",
    "        tf_df[tfidf_df.columns[i]] = tf_df[tfidf_df.columns[i]].apply(lambda x: x * (sum_of_idfs[i] / NUM_OF_DOCUMENTS))\n",
    "        \n",
    "    return tf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f14e84",
   "metadata": {},
   "source": [
    "<h2> The Experiment </h2>\n",
    "\n",
    "We run the whole pipeline on N documents from the CourtListener database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a37f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = [\"id\", \"document\"])\n",
    "if RUN_TRAIN:\n",
    "    i = 0\n",
    "    for file_name in [file for file in os.listdir(\"data/train/\") if file.endswith('.json')]:\n",
    "        try:\n",
    "            print(i)\n",
    "            document = get_document(\"data/train/\" + file_name)\n",
    "            sorted_sentences = apply_textrank(document)\n",
    "            df.loc[i] = lemmatize(sorted_sentences)\n",
    "            df.loc[i] = [file_name, lemmatize(sorted_sentences)]\n",
    "            i += 1\n",
    "        except Exception as e:\n",
    "            i += 1\n",
    "            continue\n",
    "else:\n",
    "    df = pd.read_csv(\"train_textrank.csv\", sep='\\t')[['id', 'document']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd2695e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174995.json</td>\n",
       "      <td>09 1504 united states of america appellee v. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174996.json</td>\n",
       "      <td>in this case the district court instruct the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175074.json</td>\n",
       "      <td>the bia affirm the april 3 2008 opinion of an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>175075.json</td>\n",
       "      <td>moreover fia 's affidavit explicitly confirm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175076.json</td>\n",
       "      <td>objection your honor to the line of questioni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>198335.json</td>\n",
       "      <td>then on august 15 1994 toyota advise citi tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>198336.json</td>\n",
       "      <td>the court then say briefly that while petrone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>198337.json</td>\n",
       "      <td>upon careful review of the record appellant '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>198338.json</td>\n",
       "      <td>of medical examiners 375 u.s. 411 1964 we non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>198339.json</td>\n",
       "      <td>in this case the defendant be convict of aid ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1396 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                           document\n",
       "0     174995.json   09 1504 united states of america appellee v. ...\n",
       "1     174996.json   in this case the district court instruct the ...\n",
       "2     175074.json   the bia affirm the april 3 2008 opinion of an...\n",
       "3     175075.json   moreover fia 's affidavit explicitly confirm ...\n",
       "4     175076.json   objection your honor to the line of questioni...\n",
       "...           ...                                                ...\n",
       "1391  198335.json   then on august 15 1994 toyota advise citi tha...\n",
       "1392  198336.json   the court then say briefly that while petrone...\n",
       "1393  198337.json   upon careful review of the record appellant '...\n",
       "1394  198338.json   of medical examiners 375 u.s. 411 1964 we non...\n",
       "1395  198339.json   in this case the defendant be convict of aid ...\n",
       "\n",
       "[1396 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "17790d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test = pd.DataFrame(columns = [\"id\", \"document\"])\n",
    "if RUN_TRAIN:\n",
    "    i = 0\n",
    "    for file_name in [file for file in os.listdir(\"data/test/\") if file.endswith('.json')]:\n",
    "        try:\n",
    "            document = get_document(\"data/test/\" + file_name)\n",
    "            sorted_sentences = apply_textrank(document)\n",
    "            df_test.loc[i] = lemmatize(sorted_sentences)\n",
    "            df_test.loc[i] = [file_name, lemmatize(sorted_sentences)]\n",
    "            i += 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "else:\n",
    "    df_test = pd.read_csv(\"test_textrank.csv\", sep='\\t')[['id', 'document']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc9f482f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>198340.json</td>\n",
       "      <td>finally the government argue even if turner '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198341.json</td>\n",
       "      <td>5861(d 5871 2 be a felon in know possession o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>198342.json</td>\n",
       "      <td>receive evidence interrogate examine and cros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>198343.json</td>\n",
       "      <td>yet the high maximum set by the guideline be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198631.json</td>\n",
       "      <td>98 1710 united states appellee v. michael b. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>199125.json</td>\n",
       "      <td>see downes 182 u.s. at 380 harlan j. dissent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>199126.json</td>\n",
       "      <td>credibility determination be for the jury not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>199127.json</td>\n",
       "      <td>see e.g. manso pizarro v. secretary of health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>199129.json</td>\n",
       "      <td>the jury find for volkswagen and we reason th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>199130.json</td>\n",
       "      <td>1998 cert 1998 cert john ward llambias by app...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>419 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           document\n",
       "0    198340.json   finally the government argue even if turner '...\n",
       "1    198341.json   5861(d 5871 2 be a felon in know possession o...\n",
       "2    198342.json   receive evidence interrogate examine and cros...\n",
       "3    198343.json   yet the high maximum set by the guideline be ...\n",
       "4    198631.json   98 1710 united states appellee v. michael b. ...\n",
       "..           ...                                                ...\n",
       "414  199125.json   see downes 182 u.s. at 380 harlan j. dissent ...\n",
       "415  199126.json   credibility determination be for the jury not...\n",
       "416  199127.json   see e.g. manso pizarro v. secretary of health...\n",
       "417  199129.json   the jury find for volkswagen and we reason th...\n",
       "418  199130.json   1998 cert 1998 cert john ward llambias by app...\n",
       "\n",
       "[419 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f6935a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"train_textrank.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "63aa6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"test_textrank.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f4db0a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1396\n"
     ]
    }
   ],
   "source": [
    "vectorizer, tfidf_df = call_vectorizer(df)\n",
    "train_gtf_idf = calculate_gtfidf(tfidf_df)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5d93f24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "00          1.645593\n",
       "000        16.412672\n",
       "001         0.000114\n",
       "001b        0.051524\n",
       "005         0.000197\n",
       "             ...    \n",
       "zuleta      0.029197\n",
       "zulma       0.002448\n",
       "zuluaga     0.013546\n",
       "zurosky     0.001466\n",
       "zyrone      0.002997\n",
       "Length: 24401, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gtf_idf.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dcb400f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cos_similarity(): \n",
    "    results = pd.DataFrame(columns = [\"verdict\", \"indexes\"])\n",
    "    for i, doc in enumerate(df_test.iloc[:, 1]): \n",
    "        tfidf_test = vectorizer.transform([doc])\n",
    "        tfidf_test = pd.DataFrame(tfidf_test.toarray(), columns=vectorizer.get_feature_names())\n",
    "        tfidf_test = calculate_gtfidf(tfidf_test)\n",
    "        distances = cosine_similarity(tfidf_test, train_gtf_idf).flatten()\n",
    "        indexes = np.argsort(distances)[::-1]\n",
    "        indexes = indexes[:100]\n",
    "        results = results.append({ \n",
    "            \"verdict\" : df_test.iloc[i, 0], \n",
    "            \"indexes\" : indexes}, ignore_index=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ad9a0e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20412/118777634.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcos_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20412/1643540331.py\u001b[0m in \u001b[0;36mcos_similarity\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mtfidf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtfidf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mtfidf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_gtfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_gtf_idf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mindexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20412/3421140600.py\u001b[0m in \u001b[0;36mcalculate_gtfidf\u001b[1;34m(tf_df)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0msum_of_idfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mtf_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtfidf_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtfidf_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msum_of_idfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mNUM_OF_DOCUMENTS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3610\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3611\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3612\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3614\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3782\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3783\u001b[0m         \"\"\"\n\u001b[1;32m-> 3784\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3786\u001b[0m         if (\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4504\u001b[0m         \u001b[1;31m# We should never get here with DataFrame value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4505\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4506\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_reindex_for_setitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4508\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_reindex_for_setitem\u001b[1;34m(value, index)\u001b[0m\n\u001b[0;32m  10769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10770\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10771\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  10772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10773\u001b[0m     \u001b[1;31m# GH#4107\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = cos_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "79a3f1e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20412/962467392.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d74bf76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"results/text_rank.csv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa3736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd76763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
